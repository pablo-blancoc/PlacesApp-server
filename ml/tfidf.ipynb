{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "255a0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2143a0",
   "metadata": {},
   "source": [
    "**Get all places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "14724c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_places():\n",
    "    places_file_dir = \"/Users/pabloblanco/Desktop/Places/server/dummies/places.json\"\n",
    "    data = {}\n",
    "    \n",
    "    with open(places_file_dir) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    places = []\n",
    "    for places_in_category in data.values():\n",
    "        for place in places_in_category:\n",
    "            places.append({\n",
    "                \"name\": place.get(\"name\", \"\"),\n",
    "                \"description\": place.get(\"description\", \"\")\n",
    "            })\n",
    "        \n",
    "    return places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a72400ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = get_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a6d1a",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d5ee71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in places:\n",
    "    place[\"name\"] = word_tokenize(place[\"name\"])\n",
    "    place[\"description\"] = word_tokenize(place[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5edf7f6",
   "metadata": {},
   "source": [
    "**Remove stopwords, convert to lower case, remove punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9d5bed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a80be8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words: list) -> list:\n",
    "    final = []\n",
    "    punctuation = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n'\"\n",
    "    \n",
    "    for word, tag in words:\n",
    "        # lowercase\n",
    "        word = word.lower()\n",
    "        \n",
    "        # remove stopwords\n",
    "        if word in stop_words or word in stop_words_es:\n",
    "            continue\n",
    "            \n",
    "        # remove punctuation\n",
    "        for letter in punctuation:\n",
    "            word = word.replace(letter, \"\")\n",
    "        \n",
    "        # remove one-letter words\n",
    "        if len(word) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # lemmatize word\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        if pos is None:\n",
    "            continue\n",
    "            \n",
    "        word = lemmatizer.lemmatize(word, pos)\n",
    "        \n",
    "        final.append(word)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d159a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in places:\n",
    "    # preprocess description\n",
    "    words = nltk.pos_tag(place[\"description\"])\n",
    "    place[\"description\"] = preprocess(words)\n",
    "    \n",
    "    # preprocess title\n",
    "    words = nltk.pos_tag(place[\"name\"])\n",
    "    place[\"name\"] = preprocess(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0d4c2",
   "metadata": {},
   "source": [
    "**Create dictionary of document frequency (DF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "87316ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for i in range(len(places)):\n",
    "    tokens = places[i][\"description\"]\n",
    "    tokens.extend(places[i][\"name\"])\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            df[w].add(i)\n",
    "        except KeyError:\n",
    "            df[w] = {i}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dadcf",
   "metadata": {},
   "source": [
    "df doesn't need the id's of the documents where the word is present. Hence we just take the total number of times it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fad4fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in df.keys():\n",
    "    df[word] = len(df[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fa4fe88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = df.keys()\n",
    "total_vocab_size = len(total_vocab)\n",
    "N = len(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "95df176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    freq = 0\n",
    "    try:\n",
    "        freq = df[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "29455d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_text = {}\n",
    "tf_idf_title = {}\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c648fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each place calculate tf-idf for description\n",
    "for i, place in enumerate(places):\n",
    "    \n",
    "    # get tokens of that place\n",
    "    tokens = place[\"description\"]\n",
    "\n",
    "    # get word counts per document and total words\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    # for each word in document\n",
    "    for token in np.unique(tokens):    \n",
    "        \n",
    "        # get term-frequency\n",
    "        tf = counter[token]/words_count\n",
    "        \n",
    "        # get inverse document frequency\n",
    "        idf = np.log(N/(doc_freq(token)+1))\n",
    "        \n",
    "        tf_idf_text[i, token] = tf*idf * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e4ddaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each place calculate tf-idf for title\n",
    "for i, place in enumerate(places):\n",
    "    \n",
    "    # get tokens of that place\n",
    "    tokens = place[\"description\"]\n",
    "\n",
    "    # get word counts per document and total words\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    # for each word in document\n",
    "    for token in np.unique(tokens):    \n",
    "        \n",
    "        # get term-frequency\n",
    "        tf = counter[token]/words_count\n",
    "        \n",
    "        # get inverse document frequency\n",
    "        idf = np.log(N/(doc_freq(token)+1))\n",
    "        \n",
    "        tf_idf_title[i, token] = tf*idf * (1-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1d66aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "for key in tf_idf_text.keys():\n",
    "    tf_idf[key] = tf_idf_text[key] + tf_idf_title[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a385e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(query):\n",
    "    tokenized_query = word_tokenize(query)\n",
    "    words = nltk.pos_tag(tokenized_query)\n",
    "    tokens = preprocess(words)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for i in query_weights[:10]:\n",
    "        results.append(i[0])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "90015b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 21, 13, 63, 9, 0, 37, 54, 48]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_score(\"good drinks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894f275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
