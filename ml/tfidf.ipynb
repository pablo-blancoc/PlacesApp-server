{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "255a0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2143a0",
   "metadata": {},
   "source": [
    "**Get all places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14724c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_places():\n",
    "    places_file_dir = \"/Users/pabloblanco/Desktop/Places/server/dummies/places.json\"\n",
    "    data = {}\n",
    "    \n",
    "    with open(places_file_dir) as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    places = []\n",
    "    for places_in_category in data.values():\n",
    "        for place in places_in_category:\n",
    "            places.append({\n",
    "                \"name\": place.get(\"name\", \"\"),\n",
    "                \"description\": place.get(\"description\", \"\")\n",
    "            })\n",
    "        \n",
    "    return places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4391abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_places():\n",
    "    \"\"\"\n",
    "    Reads all places from json file and creates dict object containing them\n",
    "    \"\"\"\n",
    "    places_file = \"/Users/pabloblanco/Desktop/Places/server/ml/places.json\"\n",
    "    data = {}\n",
    "\n",
    "    with open(places_file) as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    places = data[\"places\"]\n",
    "\n",
    "    return places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a72400ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = read_all_places()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a6d1a",
   "metadata": {},
   "source": [
    "**Tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d5ee71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in places:\n",
    "    place[\"name\"] = word_tokenize(place[\"name\"])\n",
    "    place[\"description\"] = word_tokenize(place[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5edf7f6",
   "metadata": {},
   "source": [
    "**Remove stopwords, convert to lower case, remove punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9d5bed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a80be8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words: list) -> list:\n",
    "    final = []\n",
    "    punctuation = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n'\"\n",
    "    \n",
    "    for word, tag in words:\n",
    "        # lowercase\n",
    "        word = word.lower()\n",
    "        \n",
    "        # remove stopwords\n",
    "        if word in stop_words or word in stop_words_es:\n",
    "            continue\n",
    "            \n",
    "        # remove punctuation\n",
    "        for letter in punctuation:\n",
    "            word = word.replace(letter, \"\")\n",
    "        \n",
    "        # remove one-letter words\n",
    "        if len(word) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # lemmatize word\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        if pos is None:\n",
    "            continue\n",
    "            \n",
    "        word = lemmatizer.lemmatize(word, pos)\n",
    "        \n",
    "        final.append(word)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d159a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in places:\n",
    "    # preprocess description\n",
    "    words = nltk.pos_tag(place[\"description\"])\n",
    "    place[\"description\"] = preprocess(words)\n",
    "    \n",
    "    # preprocess title\n",
    "    words = nltk.pos_tag(place[\"name\"])\n",
    "    place[\"name\"] = preprocess(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0d4c2",
   "metadata": {},
   "source": [
    "**Create dictionary of document frequency (DF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "87316ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for i in range(len(places)):\n",
    "    tokens = places[i][\"description\"]\n",
    "    tokens.extend(places[i][\"name\"])\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            df[w].add(i)\n",
    "        except KeyError:\n",
    "            df[w] = {i}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dadcf",
   "metadata": {},
   "source": [
    "df doesn't need the id's of the documents where the word is present. Hence we just take the total number of times it occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fad4fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in df.keys():\n",
    "    df[word] = len(df[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fa4fe88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = list(df.keys())\n",
    "total_vocab_size = len(total_vocab)\n",
    "N = len(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "95df176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    freq = 0\n",
    "    try:\n",
    "        freq = df[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "29455d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_text = {}\n",
    "tf_idf_title = {}\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c648fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each place calculate tf-idf for description\n",
    "for i, place in enumerate(places):\n",
    "    \n",
    "    # get tokens of that place\n",
    "    tokens = place[\"description\"]\n",
    "\n",
    "    # get word counts per document and total words\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    # for each word in document\n",
    "    for token in np.unique(tokens):    \n",
    "        \n",
    "        # get term-frequency\n",
    "        tf = counter[token]/words_count\n",
    "        \n",
    "        # get inverse document frequency\n",
    "        idf = np.log(N/(doc_freq(token)+1))\n",
    "        \n",
    "        tf_idf_text[i, token] = tf*idf * alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e4ddaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each place calculate tf-idf for title\n",
    "for i, place in enumerate(places):\n",
    "    \n",
    "    # get tokens of that place\n",
    "    tokens = place[\"name\"]\n",
    "\n",
    "    # get word counts per document and total words\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    # for each word in document\n",
    "    for token in np.unique(tokens):    \n",
    "        \n",
    "        # get term-frequency\n",
    "        tf = counter[token]/words_count\n",
    "        \n",
    "        # get inverse document frequency\n",
    "        idf = np.log(N/(doc_freq(token)+1))\n",
    "        \n",
    "        tf_idf_title[i, token] = tf*idf * (1-alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1d66aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total tf_idf of all words of all documents\n",
    "tf_idf = {}\n",
    "for key in tf_idf_text.keys():\n",
    "    tf_idf[key] = tf_idf_text[key] + tf_idf_title.pop(key, 0)\n",
    "for key in tf_idf_title.keys():\n",
    "    tf_idf[key] = tf_idf_title.get(key, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a385e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(query):\n",
    "    tokenized_query = word_tokenize(query)\n",
    "    words = nltk.pos_tag(tokenized_query)\n",
    "    tokens = preprocess(words)\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for i in query_weights[:10]:\n",
    "        results.append(i[0])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "90015b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 41, 36, 71, 32, 54, 31, 12, 64, 6]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_score(\"best sushi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f9fc3",
   "metadata": {},
   "source": [
    "**Implement cosine similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "728c6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((N, total_vocab_size))\n",
    "for word in tf_idf:\n",
    "    try:\n",
    "        index = total_vocab.index(word[1])\n",
    "        matrix[word[0]][index] = tf_idf[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4eb86f",
   "metadata": {},
   "source": [
    "**Create DataFrame with data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "91d7c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame(matrix)\n",
    "\n",
    "# Set index as objectId\n",
    "data_frame[\"objectId\"] = pd.Series([x[\"id\"] for x in places])\n",
    "data_frame = data_frame.set_index(\"objectId\", drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f9c0d68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rustic</th>\n",
       "      <th>mediterraneanstyle</th>\n",
       "      <th>dining</th>\n",
       "      <th>room</th>\n",
       "      <th>checkered</th>\n",
       "      <th>tablecloth</th>\n",
       "      <th>specialise</th>\n",
       "      <th>pizza</th>\n",
       "      <th>pasta</th>\n",
       "      <th>trattoria</th>\n",
       "      <th>...</th>\n",
       "      <th>accept</th>\n",
       "      <th>crema</th>\n",
       "      <th>di</th>\n",
       "      <th>crowded</th>\n",
       "      <th>still</th>\n",
       "      <th>snack</th>\n",
       "      <th>mega</th>\n",
       "      <th>hotdog</th>\n",
       "      <th>reasonable</th>\n",
       "      <th>popeye</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>objectId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RikovbyDNm</th>\n",
       "      <td>0.098846</td>\n",
       "      <td>0.098846</td>\n",
       "      <td>0.087788</td>\n",
       "      <td>0.098846</td>\n",
       "      <td>0.098846</td>\n",
       "      <td>0.098846</td>\n",
       "      <td>0.098846</td>\n",
       "      <td>0.073856</td>\n",
       "      <td>0.061038</td>\n",
       "      <td>1.367365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jsBHY6yy0H</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WkXldk0zqP</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6vwlMUoQHC</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qzknzFsH7g</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t6WNqDjjES</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yOTDvpqdsP</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oYozICIEez</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uWnJDy7xKr</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038832</td>\n",
       "      <td>0.884512</td>\n",
       "      <td>0.884512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kjWjnoqEvf</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.083639</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>0.041819</td>\n",
       "      <td>1.310339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 805 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              rustic  mediterraneanstyle    dining      room  checkered  \\\n",
       "objectId                                                                  \n",
       "RikovbyDNm  0.098846            0.098846  0.087788  0.098846   0.098846   \n",
       "jsBHY6yy0H  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "WkXldk0zqP  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "6vwlMUoQHC  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "qzknzFsH7g  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "...              ...                 ...       ...       ...        ...   \n",
       "t6WNqDjjES  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "yOTDvpqdsP  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "oYozICIEez  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "uWnJDy7xKr  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "kjWjnoqEvf  0.000000            0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "            tablecloth  specialise     pizza     pasta  trattoria  ...  \\\n",
       "objectId                                                           ...   \n",
       "RikovbyDNm    0.098846    0.098846  0.073856  0.061038   1.367365  ...   \n",
       "jsBHY6yy0H    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "WkXldk0zqP    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "6vwlMUoQHC    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "qzknzFsH7g    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "...                ...         ...       ...       ...        ...  ...   \n",
       "t6WNqDjjES    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "yOTDvpqdsP    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "oYozICIEez    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "uWnJDy7xKr    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "kjWjnoqEvf    0.000000    0.000000  0.000000  0.000000   0.000000  ...   \n",
       "\n",
       "              accept     crema        di   crowded     still     snack  \\\n",
       "objectId                                                                 \n",
       "RikovbyDNm  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "jsBHY6yy0H  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "WkXldk0zqP  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6vwlMUoQHC  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "qzknzFsH7g  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "t6WNqDjjES  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "yOTDvpqdsP  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "oYozICIEez  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "uWnJDy7xKr  0.038832  0.884512  0.884512  0.000000  0.000000  0.000000   \n",
       "kjWjnoqEvf  0.000000  0.000000  0.000000  0.041819  0.041819  0.041819   \n",
       "\n",
       "                mega    hotdog  reasonable    popeye  \n",
       "objectId                                              \n",
       "RikovbyDNm  0.000000  0.000000    0.000000  0.000000  \n",
       "jsBHY6yy0H  0.000000  0.000000    0.000000  0.000000  \n",
       "WkXldk0zqP  0.000000  0.000000    0.000000  0.000000  \n",
       "6vwlMUoQHC  0.000000  0.000000    0.000000  0.000000  \n",
       "qzknzFsH7g  0.000000  0.000000    0.000000  0.000000  \n",
       "...              ...       ...         ...       ...  \n",
       "t6WNqDjjES  0.000000  0.000000    0.000000  0.000000  \n",
       "yOTDvpqdsP  0.000000  0.000000    0.000000  0.000000  \n",
       "oYozICIEez  0.000000  0.000000    0.000000  0.000000  \n",
       "uWnJDy7xKr  0.000000  0.000000    0.000000  0.000000  \n",
       "kjWjnoqEvf  0.083639  0.041819    0.041819  1.310339  \n",
       "\n",
       "[75 rows x 805 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cols = {}\n",
    "for i, word in enumerate(total_vocab):\n",
    "    new_cols.setdefault(i, word)\n",
    "\n",
    "data_frame.rename(columns=new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9ce742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str) -> list:\n",
    "    \"\"\"Preprocess a sentence by next steps:\n",
    "        1. Getting tokens\n",
    "        2. Turning them to lower case\n",
    "        3. Removing stopwords (english and spanish)\n",
    "        4. Removing punctuation\n",
    "        5. Removing one letter words\n",
    "        6. Lemmatizing the words to get root\n",
    "\n",
    "    Args:\n",
    "        sentence (str): [description]\n",
    "\n",
    "    Returns:\n",
    "        list: [description]\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    words = nltk.pos_tag(tokens)\n",
    "\n",
    "    final = []\n",
    "    punctuation = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n'\"\n",
    "\n",
    "    for word, tag in words:\n",
    "        # lowercase\n",
    "        word = word.lower()\n",
    "\n",
    "        # remove stopwords\n",
    "        if word in stop_words or word in stop_words_es:\n",
    "            continue\n",
    "\n",
    "        # remove punctuation\n",
    "        for letter in punctuation:\n",
    "            word = word.replace(letter, \"\")\n",
    "\n",
    "        # remove one-letter words\n",
    "        if len(word) <= 1:\n",
    "            continue\n",
    "\n",
    "        # lemmatize word\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        if pos is None:\n",
    "            continue\n",
    "\n",
    "        word = lemmatizer.lemmatize(word, pos)\n",
    "\n",
    "        final.append(word)\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9104353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(text: str):\n",
    "\n",
    "    tokens = preprocess_sentence(text)\n",
    "    \n",
    "    V = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = total_vocab_size\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N)/(df+1))\n",
    "\n",
    "        try:\n",
    "            index = total_vocab.index(token)\n",
    "            V[index] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3e8cc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "87cc6136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_search(query):\n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(query)\n",
    "    \n",
    "    for index, row in data_frame.iterrows():\n",
    "        d_cosines.append(cosine_similarity(np.array(query_vector), (np.array(row))))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()\n",
    "    out = out[::-1][:10]\n",
    "    \n",
    "    return [data_frame.index[x] for x in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5b900233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XFwBIOBqFk',\n",
       " '6GDrn0l3nd',\n",
       " 'tHtbefDJ6o',\n",
       " 'WkXldk0zqP',\n",
       " 'olBnrE0XAv',\n",
       " 'yOTDvpqdsP',\n",
       " 'FpGYTCEEf0',\n",
       " 'XYtnuOvOrU',\n",
       " 'CTx0UwvcUT',\n",
       " 'Iu8Kqcbq9M']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_search(\"best sushi and place. Good beer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d8ab4cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RikovbyDNm', 'jsBHY6yy0H', 'WkXldk0zqP', '6vwlMUoQHC', 'qzknzFsH7g',\n",
       "       '4PWS5AlhBS', 'AhoMtdIvVQ', '67G7wVmxiH', 'tgbWcG5bcP', 'sx23GQkGS9',\n",
       "       'uCg01ne0rU', 'mqTLx15zKL', 'olBnrE0XAv', 'hIAC9EYb18', 'zYeJ9hXCmV',\n",
       "       'hlmD7YXRSZ', 'dFXSLkA5GH', 'FRzeo68LSt', '7MZDYdeQ90', 'ar0HuYZEJq',\n",
       "       't4JVRsh7F2', 'uocGLUaNid', 'uPDdTM1vjO', 'HpJkHNZ5u8', 'nz1YWNPJHA',\n",
       "       'hCOeNgcDVU', 'gnnVYOd2Vx', 'i3LndVCgud', 'NzPSN7A2aa', '1znr7xjJmc',\n",
       "       'zpbaAHJPh3', 'reTyTA1JlH', 'Iu8Kqcbq9M', 'qFnXU8tVmq', '7rzHCPMaMn',\n",
       "       '60aXlIln71', 'tHtbefDJ6o', 'N6CgzqclRR', 'MHXR6u2JJQ', 'WXt6KBXtIF',\n",
       "       'HuadGVieXe', '6GDrn0l3nd', 'SbrxQe0Y7u', 'DMMb9IhAew', 'kMyV93ysyk',\n",
       "       'HBa1yXo31Z', 'XFwBIOBqFk', '7a0cDFsv8P', 'sZJTdHPQEG', 'CMmwPQEuwD',\n",
       "       '1a7o93N171', 'z2Xwml9PqV', '6oo32HUEgT', 'NKqrZHcwOK', '2roHEcjYRG',\n",
       "       'O7pO1kcNPa', 'kGqDgwqvOs', 'CTx0UwvcUT', 'lfbqwdy2Dv', 'CHFk6CCry5',\n",
       "       'B7LNsbs2UD', 'V04masGKPI', 'FpGYTCEEf0', '0m7LCaGjm8', 'XYtnuOvOrU',\n",
       "       'jJRCxHEqX8', 'hirY8yQH2A', 'BJFFmfmq3V', 'xrsUlaiLut', 'fh57eEk2C8',\n",
       "       't6WNqDjjES', 'yOTDvpqdsP', 'oYozICIEez', 'uWnJDy7xKr', 'kjWjnoqEvf'],\n",
       "      dtype='object', name='objectId')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6749c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
